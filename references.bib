@article{Zuluaga-Gomez2023Virtual,
  title        = {A Virtual Simulation‑Pilot Agent for Training of Air Traffic Controllers},
  author       = {Zuluaga‑Gomez, Juan and Prasad, Amrutha and Nigmatulina, Iuliia and Motlicek, Petr and Kleinert, Matthias},
  journal      = {arXiv preprint},
  volume       = {arXiv:2304.07842},
  year         = {2023},
  url          = {https://arxiv.org/abs/2304.07842},
  eprint       = {2304.07842},
  archivePrefix= {arXiv},
  primaryClass = {eess.AS}
}
@online{WhatRadioEquipmentPilotsUseToCommunicateWithATC2018,
  author       = {joey},
  title        = {What radio equipment do pilots use to communicate with ATC?},
  year         = {2018},
  url          = {https://aviation.stackexchange.com/questions/50366/what-radio-equipment-do-pilots-use-to-communicate-with-atc},
  note         = {Aviation StackExchange, asked April 9, 2018, accessed <insert date of access>},
}
@article{ZuluagaGomez2023Lessons,
  title        = {Lessons Learned in Transcribing 5000 h of Air Traffic Control Communications for Robust Automatic Speech Understanding},
  author       = {Zuluaga‑Gomez, Juan and Nigmatulina, Iuliia and Prasad, Amrutha and Motlicek, Petr and Khalil, Driss and Madikeri, Srikanth and Tart, Allan and Szoke, Igor and Lenders, Vincent and Rigault, Mickael and Choukri, Khalid},
  journal      = {Aerospace},
  year         = {2023},
  volume       = {10},
  number       = {10},
  pages        = {898},
  doi          = {10.3390/aerospace10100898},
  url          = {https://publications.idiap.ch/attachments/papers/2023/Juan_AEROSPACE_2023.pdf}
}
@techreport{ChenKopaldMaTarakanWei2021ATCSpeechRecognition,
  title        = {Air Traffic Control Speech Recognition},
  author       = {Chen, Shuo and Kopald, Hunter and Ma, Weiye and Tarakan, Robert and Wei, Yuan‑Jun},
  institution  = {The MITRE Corporation},
  year         = {2021},
  type         = {Draft Technical Report},
  month        = {8},
  url          = {https://www.haawaii.de/wp/wp-content/uploads/2021/08/ATC-speech-recognition-The-MITRE-Corporation-draft-August-2021.pdf},
  note         = {Approved for Public Release, Distribution Unlimited. Case Number 21‑0755}
}
@article{WeeEtAl2025Adapting,
  title        = {Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications},
  author       = {Wee, Marcus Yu Zhe and Wong, Justin Juin Hng and Lim, Lynus and Tan, Joe Yu Wei and Gupta, Prannaya and Lim, Dillion and Tew, En Hao and Han, Aloysius Keng Siew and Lim, Yong Zhi},
  journal      = {arXiv preprint},
  volume       = {arXiv:2502.20311},
  year         = {2025},
  url          = {https://arxiv.org/abs/2502.20311},
  eprint       = {2502.20311},
  archivePrefix= {arXiv},
  primaryClass = {cs.LG}
}
@article{ZuluagaGomez2024ATCO2,
  title        = {ATCO2 corpus: A Large‑Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications},
  author       = {Zuluaga‑Gomez, Juan Pablo and Veselý, Karel and Szöke, Igor and Blatt, Alexander and Motlicek, Petr and Kocour, Martin and Rigault, Mickael and Choukri, Khalid and Prasad, Amrutha and Sarfjoo, Saeed and Nigmatulina, Iuliia and Cevenini, Claudia and Kolčárek, Pavel and Tart, Allan},
  journal      = {arXiv preprint / SSRN / IDIAP publications},
  year         = {2024},
  url          = {https://www.fit.vut.cz/research/group/speech/public/publi/2024/zuluaga-Gomez_2024_ATCO2_paper_final.pdf},
  note         = {Submitted version; also available via SSRN and IDIAP}  
}
@article{Fan2024CustomizationASR,
  title        = {Customization of the ASR System for ATC Speech with Improved Fusion Method},
  author       = {Fan, J. and others},
  journal      = {Aerospace},
  year         = {2024},
  volume       = {11},
  number       = {3},
  pages        = {219},
  doi          = {10.3390/aerospace11030219},
  url          = {https://www.mdpi.com/2226-4310/11/3/219}
}
@article{ZuluagaGomez2021Contextual,
  title        = {Contextual Semi‑Supervised Learning: An Approach To Leverage Air‑Surveillance and Untranscribed ATC Data in ASR Systems},
  author       = {Zuluaga‑Gomez, Juan Pablo and Nigmatulina, Iuliia and Prasad, Amrutha and Motlicek, Petr and Veselý, Karel and Kocour, Martin and Szöke, Igor},
  journal      = {arXiv preprint},
  volume       = {arXiv:2104.03643},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.03643},
  eprint       = {2104.03643},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL}
}
@online{ATCO2_EndToEndCallsignRecognition2021,
  author       = {Motlicek, Petr},
  title        = {End to End Callsign Recognition System},
  year         = {2021},
  month        = {5},
  url          = {https://www.atco2.org/news/end-to-end-callsign-recognition-system},
  note         = {ATCO2 blog, accessed \today}
}
@online{SimpleFlying_NonEnglishATCPilotCommunicationsGuide,
  author       = {SimpleFlying},
  title        = {Non‑English ATC Pilot Communications Guide},
  url          = {https://simpleflying.com/non-english-atc-pilot-communications-guide/},
  note         = {Accessed: \today}
}
@article{wee2024adapting,
  title        = {Adapting Automatic Speech Recognition for Accented Air Traffic Control Communications},
  author       = {Wee, Marcus Yu Zhe and Wong, Justin Juin Hng and Lim, Lynus and Tan, Joe Yu Wei and Gupta, Prannaya and Lim, Dillion and Tew, En Hao and Han, Aloysius Keng Siew and Lim, Yong Zhi},
  journal      = {arXiv preprint arXiv:2502.20311},
  year         = {2024},
  url          = {https://arxiv.org/abs/2502.20311},
  archivePrefix= {arXiv},
  eprint       = {2502.20311},
  primaryClass = {cs.CL}
}
@misc{Jzuluaga_uwb_atcc,
  title        = {UWB‑ATCC Corpus: Communication between Pilots and Air Traffic Controllers},
  author       = {Zuluaga‑Gomez, Juan Pablo and others},
  howpublished = {HuggingFace Datasets, Jzuluaga/uwb\_atcc},
  year         = {2023},
  note         = {Accessed: \today, license: CC BY‑NC‑SA 4.0},
  url          = {https://huggingface.co/datasets/Jzuluaga/uwb\_atcc}
}
@inproceedings{hofbauer-etal-2008-atcosim,
    title = "The {ATCOSIM} Corpus of Non-Prompted Clean Air Traffic Control Speech",
    author = "Hofbauer, Konrad  and
      Petrik, Stefan  and
      Hering, Horst",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = 5,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L08-1507/",
    abstract = "Air traffic control (ATC) is based on voice communication between pilots and controllers and uses a highly task and domain specific language. Due to this very reason, spoken language technologies for ATC require domain-specific corpora, of which only few exist to this day. The ATCOSIM Air Traffic Control Simulation Speech corpus is a speech database of non-prompted and clean ATC operator speech. It consists of ten hours of speech data, which were recorded in typical ATC control room conditions during ATC real-time simulations. The database includes orthographic transcriptions and additional information on speakers and recording sessions. The ATCOSIM corpus is publicly available and provided online free of charge. In this paper, we first give an overview of ATC related corpora and their shortcomings. We then show the difficulties in obtaining operational ATC speech recordings and propose the use of existing ATC real-time simulations. We describe the recording, transcription, production and validation process of the ATCOSIM corpus, and outline an application example for automatic speech recognition in the ATC domain."
}
@misc{malorca_project,
  title = {{MALORCA Project – Machine Learning of Speech Recognition Models for Controller Assistance}},
  author = {{German Aerospace Center (DLR) and Saarland University (USAAR) and Idiap Research Institute (Idiap) and Austro Control Österreichische Gesellschaft für Zivilluftfahrt mit beschränkter Haftung (ACG) and Air Navigation Services of the Czech Republic (ANS CR)}},
  year = {2018},
  howpublished = {Web Page},
  url = {https://www.malorca-project.de/},
  note = {Accessed: 2025-09-25}
}
@inproceedings{panayotov2015librispeech,
  title={{Librispeech: an ASR corpus based on public domain audio books}},
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5206--5210},
  year={2015},
  organization={IEEE}
}
@inproceedings{hernandez2018ted,
  title={{A new TED-LIUM release (release 3) for speech recognition}},
  author={Hernandez, Fran{\c{c}}ois and Nguyen, Vincent and Garc{\'\i}a, Pablo and Est{\`e}ve, Yannick and Carr{\'e}, Micka{\"e}l},
  booktitle={International Conference on Speech and Computer},
  pages={178--186},
  year={2018},
  organization={Springer}
}
@inproceedings{ardila2020common,
  title={{Common Voice: A Massively-Multilingual Speech Corpus}},
  author={Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M. and Weber, Gregor},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={4218--4222},
  year={2020}
}
@inproceedings{wang-etal-2021-voxpopuli,
    title = "{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation",
    author = "Wang, Changhan  and
      Riviere, Morgane  and
      Lee, Ann  and
      Wu, Anne  and
      Talnikar, Chaitanya  and
      Haziza, Daniel  and
      Williamson, Mary  and
      Pino, Juan  and
      Dupoux, Emmanuel",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = 8,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.80",
    doi = "10.18653/v1/2021.acl-long.80",
    pages = "993--1003",
}
@article{DBLP:journals/corr/abs-2006-11477,
  author       = {Alexei Baevski and
                  Henry Zhou and
                  Abdelrahman Mohamed and
                  Michael Auli},
  title        = {wav2vec 2.0: {A} Framework for Self-Supervised Learning of Speech
                  Representations},
  journal      = {CoRR},
  volume       = {abs/2006.11477},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.11477},
  eprinttype    = {arXiv},
  eprint       = {2006.11477},
  timestamp    = {Tue, 23 Jun 2020 17:57:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-11477.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Pratap2020MLSAL,
  title={MLS: A Large-Scale Multilingual Dataset for Speech Research},
  author={Vineel Pratap and Qiantong Xu and Anuroop Sriram and Gabriel Synnaeve and Ronan Collobert},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.03411}
}
@misc{eurocontrol_icao_nodate,
	title = {{ICAO} Standard Phraseology: A Quick Reference Guide for Commercial Air Transport Pilots},
	author = {{EUROCONTROL}},
	howpublished = {\url{https://skybrary.aero/sites/default/files/bookshelf/115.pdf}},
	note = {Accessed: 2023-10-27}
}
@misc{callsign_recognition,
author = {Blatt, Alexander and Kocour, Martin and Veselý, Karel and Szöke, Igor and Klakow, Dietrich},
year = {2022},
month = {04},
pages = {},
title = {Call-sign recognition and understanding for noisy air-traffic transcripts using surveillance information},
doi = {10.48550/arXiv.2204.06309}
}

@Article{aerospace12050376,
AUTHOR = {Chen, Sheng and Pan, Weijun and Wang, Yidi and Chen, Shenhao and Wang, Xuan},
TITLE = {Research on the Method of Air Traffic Control Instruction Keyword Extraction Based on the Roberta-Attention-BiLSTM-CRF Model},
JOURNAL = {Aerospace},
VOLUME = {12},
YEAR = {2025},
NUMBER = {5},
ARTICLE-NUMBER = {376},
URL = {https://www.mdpi.com/2226-4310/12/5/376},
ISSN = {2226-4310},
DOI = {10.3390/aerospace12050376}
}
@inbook{doi:10.2514/6.2024-4359,
author = {Hillel A. Steinmetz and Jacob Tao and Stephen S. Clarke and Krishna Kalyanam},
title = {A Natural Language Understanding Approach for Digitizing Aircraft Ground Taxi Instructions},
booktitle = {AIAA AVIATION FORUM AND ASCEND 2024},
chapter = {},
pages = {},
doi = {10.2514/6.2024-4359},
URL = {https://arc.aiaa.org/doi/abs/10.2514/6.2024-4359},
eprint = {https://arc.aiaa.org/doi/pdf/10.2514/6.2024-4359},
    abstract = { Advancements in natural language processing (NLP) technologies offer a unique opportunity to furnish aircraft crews, primarily pilots, with digital instructions for taxiing operations. Digital taxi instructions, delivered either as text or graphics, can streamline taxiing procedures, thereby reducing radio congestion, minimizing communication errors, and enhancing aircraft monitoring. Techniques used for natural language understanding (NLU), a subset of NLP focused on machine comprehension of natural language, can extract taxi instructions directly from verbal radio communications. This capability paves the way for implementing a digital taxi communication framework with minimal adjustments to the existing air traffic controller operations. This paper delves into a novel application of NLU: the automated generation of digital taxi instructions from air traffic controller speech. We detail the development of an annotation scheme to represent aircraft ground traffic communications within the US National Airspace System (NAS), employing intent classification (IC) and slot filling (SF) to extract taxi instructions using NLU models. Several neural network models were trained on a dataset annotated with our scheme, achieving notable accuracy and F1 scores. Our research demonstrates the feasibility of using NLU to automatically generate digital taxi instructions, showcasing its potential to streamline the implementation of digital taxi communications. }
}
@misc{eurocontrol:remote-piloting-2021,
  organization = {EUROCONTROL},
  title      = {New remote piloting function in {ESCAPE}, the {EUROCONTROL} Innovation Hub {ATC} simulator, adds greater flexibility for stakeholders},
  howpublished = {\url{https://www.eurocontrol.int/news/new-remote-piloting-function-escape-eurocontrol-innovation-hub-atc-simulator-adds-greater}},
  month      = oct,
  year       = {2021},
  note       = {Accessed: 2025-10-19}
}
@misc{MicroNavBestTower2025,
  title        = {{BEST Tower - Advanced ATC Simulator Training}},
  author       = {{Micro Nav}},
  howpublished = {\url{https://www.micronav.co.uk/best-tower/}},
  year         = {2025},
  note         = {Accessed: 2025-10-20}
}
@misc{CSSOFTATCSimulator2025,
  title        = {{ATC Simulator}},
  author       = {{CS SOFT a.s.}},
  howpublished = {\url{https://www.cs-soft.cz/en/atc-simulator}},
  year         = {2025},
  note         = {Accessed: 2025-10-20}
}
@misc{HAVELSANAtcTRsim2021,
  title        = {{Webinar: AtcTRsim - Air Traffic Control Training Simulator}},
  author       = {{HAVELSAN}},
  howpublished = {\url{https://www.havelsan.com/en/webinars/atctrsim}},
  month        = {mar},
  year         = {2021},
  note         = {Webinar held on 30 March 2021; Accessed: 2025-10-20}
}
@techreport{bolczak2005accommodating,
  author = {Bolczak, Richard and Celio, Joseph},
  title = {Accommodating ATC System Evolution through Advanced Training Techniques},
  institution = {MITRE},
  year = {2005},
  month = {12},
  url = {https://www.mitre.org/news-insights/publication/accommodating-atc-system-evolution-through-advanced-training-techniques},
  note = {Publication on the MITRE website}
}
@ARTICLE{LinEtAl,
  author={Lin, Yi and Wu, YuanKai and Guo, Dongyue and Zhang, Pan and Yin, Changyu and Yang, Bo and Zhang, Jianwei},
  journal={IEEE Transactions on Human-Machine Systems}, 
  title={A Deep Learning Framework of Autonomous Pilot Agent for Air Traffic Controller Training}, 
  year={2021},
  volume={51},
  number={5},
  pages={442-450},
  keywords={Training;Task analysis;Hidden Markov models;Atmospheric modeling;Speech recognition;Aircraft;Neural networks;Air traffic controller (ATCO) training;autonomous pilot agent (APA);pilot repetition generation (PRG);text-to-speech (TTS);virtual training},
  doi={10.1109/THMS.2021.3102827}}10.1109/THMS.2021.3102827}
}
@mastersthesis{vanDoorn2023,
  author       = {van Doorn, Jan Laurenszoon P. M.},
  title        = {Applying Large-Scale Weakly Supervised Automatic Speech Recognition to Air Traffic Control},
  school       = {Delft University of Technology},
  year         = {2023},
  address      = {Delft, The Netherlands},
  note         = {Master’s thesis, Faculty of Aerospace Engineering},
  url          = {https://resolver.tudelft.nl/uuid:8aa780bf-47b6-4f81-b112-29e23bc06a7d}
}
@misc{OpenAI_Whisper_2022,
  author       = {OpenAI},
  title        = {Introducing Whisper},
  year         = {2022},
  howpublished = {\url{https://openai.com/es-ES/index/whisper/}},
  note         = {Accessed: 2025-11-17}
}
@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}
@misc{cunha2025thoroughbenchmarkautomatictext,
      title={A thorough benchmark of automatic text classification: From traditional approaches to large language models}, 
      author={Washington Cunha and Leonardo Rocha and Marcos André Gonçalves},
      year={2025},
      eprint={2504.01930},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.01930}, 
}
@misc{belcak2025smalllanguagemodelsfuture,
      title={Small Language Models are the Future of Agentic AI}, 
      author={Peter Belcak and Greg Heinrich and Shizhe Diao and Yonggan Fu and Xin Dong and Saurav Muralidharan and Yingyan Celine Lin and Pavlo Molchanov},
      year={2025},
      eprint={2506.02153},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.02153}, 
}
@online{AguileraEnhancingAviation2024,
  author = {Frank Morales Aguilera},
  title = {Enhancing Aviation Expertise in Meta-Llama-3–8B: A Deeper Dive into Fine-Tuning with Expanded AviationQA},
  organization = {The Deep Hub},
  journal = {Medium},
  year = {2024},
  month = {June},
  day = {21},
  url = {https://medium.com/thedeephub/enhancing-aviation-expertise-in-meta-llama-3-8b-a-deeper-dive-into-fine-tuning-with-expanded-93c38af34d43},
  urldate = {2025-11-26}
}
@online{OpenAIGPT4omini2024,
  author = {{OpenAI}},
  title = {GPT-4o mini: Advancing Cost-Efficient Intelligence},
  url = {https://openai.com/es-ES/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
  organization = {OpenAI},
  year = {2024},
  month = {July},
  day = {18},
  urldate = {2025-11-26}
}
@inproceedings{rag,
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
title = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {793},
numpages = {16},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}